---
title: "Analyzing Tech Article Text Data"
author: "Donald Ruud"
date: '2021-08-01'
slug: analyzing-tech-article-text-data
categories: R
tags: R Markdown
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
mega_abstract <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/mega_abstract")
prc <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/finaldatasetPCA")
title_tib <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/title_tib")
abstract <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/abstractdirty")
# Summarize to the day level
title_tib_day <- title_tib %>%
  group_by(testdate2, word) %>%
  summarise(n = sum(n))
```


## Analyzing Tech Article Text Data  

The smell of sawdust and the rumble of machinery fills your ears. Your arms ache and your back hurts, everything hurts, seems like it always does these days. It's all just another day at the mill. It's hot August afternoons like these that make you wonder, "How did I end up here? I was working as data analyst and after a few years I was laid off and no one else wanted to hire me."  

Well, don't ask me. I might be the one writing the story, but I don't know your personal history. Maybe you didn't keep up on new developments in the field? Maybe you're just a pain to work with? I can't help with the latter, but we can analyze tech articles from https://technews.acm.org/ and try to identify major developments in the industry.  

The tech articles we're using describe current events related to technology, science, data, etc. We'll apply some basic text analysis techniques and then try something more complex with Principle Component Analysis. The goal of this project is to see how Data Science has changed over time from a more general tech lens.  


## High Level Results
###### *For those who don't like waiting.*  

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(lubridate)
library(plotly)
# Trying to average to the month level to tease out trends
prc_month_avg <- prc
prc_month_avg$Month_Yr <- format(as.Date(prc_month_avg$testdate2), "%Y-%m")


prc_month_avg <- prc_month_avg%>%
  group_by(Month_Yr) %>%
  summarise(Female_STEM_month = mean(Female_STEM),
            Users_Not_Microchips_month = mean(Users_Not_Microchips),
            Female_Education_Not_Cybersecurity_month = mean(Female_Education_Not_Cybersecurity),
            Cutting_Edge_Computing_month = mean(Cutting_Edge_Computing),
            Self_Driving_Cars_month = mean(Self_Driving_Cars),
            US_Cybersecurity_month = mean(US_Cybersecurity),
            UX_month = mean(UX),
            Healthcare_month = mean(Healthcare))

colors <- c("Female_STEM" = "#1b9e77", "Users_Not_Microchips" = "#d95f02", "Healthcare" = "#7570b3", "Self_Driving_Cars" = "#e7298a", "Female_Education_Not_Cybersecurity" = "#66a61e", "Cutting_Edge_Computing" = "#e6ab02", "US_Cybersecurity" = "#a6761d", "UX" = "#666666")


pc_plot <- ggplot(data = prc_month_avg, aes(x = parse_date_time(Month_Yr, "ym"), y = Female_STEM_month, group = 1)) +
  geom_smooth(aes(color = "Female_STEM"), se = FALSE) +
  geom_smooth(aes(y = Users_Not_Microchips_month, color = "Users_Not_Microchips"), se = FALSE) +
  geom_smooth(aes(y = Healthcare_month, color = "Healthcare"), se = FALSE) +
  geom_smooth(aes(y = Self_Driving_Cars_month, color = "Self_Driving_Cars"), se = FALSE) +
  geom_smooth(aes(y = Female_Education_Not_Cybersecurity_month, color = "Female_Education_Not_Cybersecurity"), se = FALSE) +
  geom_smooth(aes(y = Cutting_Edge_Computing_month, color = "Cutting_Edge_Computing"), se = FALSE) +
  geom_smooth(aes(y = US_Cybersecurity_month, color = "US_Cybersecurity"), se = FALSE) +
  geom_smooth(aes(y = UX_month, color = "UX"), se = FALSE) +
  theme_classic() +
  ylab("Category Strength") +
  xlab("Time") +
  labs(color = "Legend") +
  scale_color_manual(values = colors) +
  ggtitle("Cybersecurity Captures the Minds of Tech Writers")

ggplotly(pc_plot)
```  

There you go. This is the crown jewel of the project.  
Wait, you want me to actually explain something to you? I can't just throw up this graph to satisfy the impatient among you?  
Fine, here are the highlights:  
1. Cyber Security has been a scorching hot topic recently and looks like it may continue to be.  
2. Articles that **ignore** silicon manufacturing and **emphasize** users and identifiers have been consistently strong and appear to be getting stronger.  
3. Cutting Edge Computing was building steadily until about 2014 when everyone realized that quantum computers were still 10-15 years away and got sad.  
4. Interest in Cutting Edge Computing recently soared again and appears to have a strong upward trend in tech discussions.  

#### Time to dive into the details!

## Article Roadmap  
###### *Ha! Got you! No details just yet, but I got you this cool roadmap so you don't get lost!*

[Background]  
[Process]  
[Data Acquisition, Cleaning, and Exploration]  
[Results]  


## Background  
###### *Now we can actually get into the details.*

Data Science is a relatively new industry that has only recently begun to solidify. We can trace elements of Data Science far back into history, but only within the past few decades has computing technology advanced enough to make widespread statistical analyses feasible. Data Science as a field has shifted a lot over the past 20 years. Specifically, the way we talk about Data Science and the various concepts that are important.  

What is the problem? Currently, we base our thoughts on the “next big thing” in Data Science on theory and, for lack of a better term, hype. Theory in of itself is not a bad place to start, but it’s not always right when it comes to predicting changes in the industry. For example, the theory behind quantum computing is solid, but actually getting it to work at scale is...well we're working on it. Hype on the other hand is driven oftentimes by irrational excitement about a product, service, or concept. Due to the malleable nature of Data Science, we need to be aware of what ideas are most popular and know if there's anything useful behind them.  

We need to make sure that we are prepared as industry professionals to use and understand new technology as it emerges. But we also need to be able to cut through the passing fads and provide real, actionable solutions to our employers. Being behind the curve in Data Science can seriously hamper your capabilities and your career.  


## Process

So how are we going to solve the problem?  

1. Scrape article archives from https://technews.acm.org/ (Dec 2007 to June 2021)  
2. Do **a lot** of data cleaning  
3. Get the top 10 most used words and visualize them  
4. Spice things up with Principle Component Analysis (PCA)  

### PCA Side-bar  
###### *You can skip this section if you don't care about what PCA is or how it works*

The simplest way to think of PCA is that it tells you which variables tend to hang together. Variables which all have large positive coefficients for a given principle component exert a greater effect on that component.  

PCA is typically used for dimensionality reduction (*reducing the number variables you have*). Essentially, it takes your data and creates a series of linear equations, these are the principle components. Each principle component has coefficients for every variable in your dataset. And just like with linear equations, larger coefficients (positive and negative) define what is most important in a given principle component.  

But, I'm not trying to write a post on the intricate details of PCA so **back to the analysis!**  


## Data Acquisition, Cleaning, and Exploration

Quick markdown code walk-through (cliff notes version)  
*Still need to condense code to minimal levels. May be valuable to simply describe the cleaning process instead.*

As mentioned earlier, these data have been scraped from https://technews.acm.org/ and cover a period from December 2007 to June 2021. If you haven't scraped data or dealt with scraped data before, I'll tell you a little secret, it's **incredibly messy.** What, you don't believe me? Take a look at the raw abstracts:

```{r warning=FALSE, echo=FALSE}
head(abstract$x, 2)
```  

Yep, it's not the prettiest sight. There's a lot of cleaning to be done in order to pull out dates, titles, and the abstracts. I'm not going to bore you with a full code walk-through of all the cleaning though. Instead, let's both just enjoy the beauty of the finished version:

```{r warning=FALSE, echo=FALSE}
mega_abstract %>%
  select(cleantitle, testdate2, cleanab) %>%
  head()
```


Any major findings from exploration?  
The scraper did not do a perfect job collecting all the article information. There are occasional instances where nearly a month of data is missing.
*Go into more detail on what data is missing. Potentially explore* **why** *it's missing.*

*There appears to be significant changes over time in the main collections of words.*  
*Add graphs of trends over time*

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Find the top 10 words that appear in the most articles
top_10 <- title_tib %>%
  distinct(word, .keep_all = TRUE) %>%
  arrange(desc(total)) %>%
  head(10)
top_10
  # computer, data, researchers, ai, quantum, research, computing, science, software, tech
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create bar graph
colors <- c("#a6cee3", "#1f78b4", "#b2df8a", "#33a02c", "#fb9a99", "#e31a1c", "#fdbf6f", "#ff7f00", "#cab2d6", "#6a3d9a")

ggplot(data = top_10, aes(x = reorder(word, -total), y = total, fill = colors)) +
  geom_col() +
  theme_classic() +
  theme(legend.position = "none") +
  xlab(element_blank()) +
  ylab("Total") +
  ggtitle("Top 10 Words Across Articles From 2007 to 2021") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
        text = element_text(size = 14))
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create graph of top 10 over the entire time period
filter_words <- c("computer", "data", "researchers", "ai", "quantum", "research", "computing", "science", "software", "tech")
top_10_over_time <- title_tib_day %>%
  filter(word %in% filter_words)
# Combined Usage (smoothed)
ggplot(data = top_10_over_time, aes(x = testdate2, y = n)) +
  geom_smooth(size = 1.25, se = FALSE) +
  theme_classic() +
  xlab("Date") +
  ylab("Word Usage") +
  ggtitle("Top 10 Words Cumulative Usage Over Time")
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Separated Usage (smoothed)
ggplot(data = top_10_over_time, aes(x = testdate2, y = n)) +
  geom_smooth(aes(color = word), size = 1.25, se = FALSE) +
  scale_color_manual(values = colors) +
  theme_classic() +
  xlab("Date") +
  ylab("Word Usage") +
  ggtitle("Top 10 Words Usage Over Time")
```

![](https://i.imgur.com/9AisQbG.gif)






Include link to finished dashboard. Dashboard is not finished at this time.













## Results

Markdown code walk-through of method application  
*May consider just describing method and providing intermediate visuals*  

Discuss initial findings.
*Still in-progress. Would be a good idea to show trends over time here*

## Finished Dashboard

Display findings in polished graphical form  
*The* **cool** *graphs are still in development.*

Use R-Shiny to dev interactive dashboard with various graphs.
*The Shiny app is still under development and I'm not sure exactly how to integrate it into R-Markdown just yet.*  

Brief discussion on the results, ideally the visuals will do most of the talking.  
*Nothing to discuss just yet. I'm still digging into the results.*
