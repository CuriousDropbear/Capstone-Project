---
title: "Project Blog Post Draft"
author: "Donald Ruud"
date: '2021-07-28'
slug: project-blog-post-draft
categories: R
tags: R Markdown
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
mega_abstract <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/mega_abstract")
prc <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/finaldatasetPCA")
title_tib <- read_csv("C:/Users/Donald/Documents/Willamette University/Capstone/title_tib")
# Summarize to the day level
title_tib_day <- title_tib %>%
  group_by(testdate2, word) %>%
  summarise(n = sum(n))
```


## Project Blog Post Draft

My project is centered on Data Science articles. These articles contain information on what is currently happening in our industry and how we talk about it. We primarily use theory to determine what trends are up and coming in our industry. I believe we can approach this from a more systematic perspective. We should perform a text analysis on Data Science articles published over the past 20 years so that we can visualize the most prominent topics and better understand how our industry has developed. Through this visualization we may be able to draw critical insights about where our industry is headed.  

*There appears to be significant changes over time in the main collections of words.*  
*Add graphs of trends over time*

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Find the top 10 words that appear in the most articles
top_10 <- title_tib %>%
  distinct(word, .keep_all = TRUE) %>%
  arrange(desc(total)) %>%
  head(10)
top_10
  # computer, data, researchers, ai, quantum, research, computing, science, software, tech
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create bar graph
colors <- c("#a6cee3", "#1f78b4", "#b2df8a", "#33a02c", "#fb9a99", "#e31a1c", "#fdbf6f", "#ff7f00", "#cab2d6", "#6a3d9a")

ggplot(data = top_10, aes(x = reorder(word, -total), y = total, fill = colors)) +
  geom_col() +
  theme_classic() +
  theme(legend.position = "none") +
  xlab(element_blank()) +
  ylab("Total") +
  ggtitle("Top 10 Words Across Articles From 2007 to 2021") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
        text = element_text(size = 14))
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create graph of top 10 over the entire time period
filter_words <- c("computer", "data", "researchers", "ai", "quantum", "research", "computing", "science", "software", "tech")
top_10_over_time <- title_tib_day %>%
  filter(word %in% filter_words)
# Combined Usage (smoothed)
ggplot(data = top_10_over_time, aes(x = testdate2, y = n)) +
  geom_smooth(size = 1.25, se = FALSE) +
  theme_classic() +
  xlab("Date") +
  ylab("Word Usage") +
  ggtitle("Top 10 Words Cumulative Usage Over Time")
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Separated Usage (smoothed)
ggplot(data = top_10_over_time, aes(x = testdate2, y = n)) +
  geom_smooth(aes(color = word), size = 1.25, se = FALSE) +
  scale_color_manual(values = colors) +
  theme_classic() +
  xlab("Date") +
  ylab("Word Usage") +
  ggtitle("Top 10 Words Usage Over Time")
```

![](https://i.imgur.com/9AisQbG.gif)

```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(ggwordcloud)
library(gganimate)
# Time to try plotting
set.seed(42)
static_wc <- title_tib_day %>%
  ggplot(aes(label = word, size = n)) +
  # Try adjusting area_corr_power higher is smaller, lower is bigger
  geom_text_wordcloud_area(area_corr_power = 1) +
  scale_size_area(max_size = 24) +
  theme_classic()

dynamic_wc <- static_wc +
  transition_states(states = testdate2, state_length = 0.75)
animate(dynamic_wc, fps = 1, width = 500, height = 500, end_pause = 30)

```



```{r echo=FALSE, warning=FALSE, message=FALSE}
library(lubridate)
library(plotly)
# Trying to average to the month level to tease out trends
prc_month_avg <- prc
prc_month_avg$Month_Yr <- format(as.Date(prc_month_avg$testdate2), "%Y-%m")


prc_month_avg <- prc_month_avg%>%
  group_by(Month_Yr) %>%
  summarise(Female_STEM_month = mean(Female_STEM),
            Users_Not_Microchips_month = mean(Users_Not_Microchips),
            Female_Education_Not_Cybersecurity_month = mean(Female_Education_Not_Cybersecurity),
            Cutting_Edge_Computing_month = mean(Cutting_Edge_Computing),
            Self_Driving_Cars_month = mean(Self_Driving_Cars),
            US_Cybersecurity_month = mean(US_Cybersecurity),
            UX_month = mean(UX),
            Healthcare_month = mean(Healthcare))

colors <- c("Female_STEM" = "#1b9e77", "Users_Not_Microchips" = "#d95f02", "Healthcare" = "#7570b3", "Self_Driving_Cars" = "#e7298a", "Female_Education_Not_Cybersecurity" = "#66a61e", "Cutting_Edge_Computing" = "#e6ab02", "US_Cybersecurity" = "#a6761d", "UX" = "#666666")


pc_plot <- ggplot(data = prc_month_avg, aes(x = parse_date_time(Month_Yr, "ym"), y = Female_STEM_month, group = 1)) +
  geom_smooth(aes(color = "Female_STEM"), se = FALSE) +
  geom_smooth(aes(y = Users_Not_Microchips_month, color = "Users_Not_Microchips"), se = FALSE) +
  geom_smooth(aes(y = Healthcare_month, color = "Healthcare"), se = FALSE) +
  geom_smooth(aes(y = Self_Driving_Cars_month, color = "Self_Driving_Cars"), se = FALSE) +
  geom_smooth(aes(y = Female_Education_Not_Cybersecurity_month, color = "Female_Education_Not_Cybersecurity"), se = FALSE) +
  geom_smooth(aes(y = Cutting_Edge_Computing_month, color = "Cutting_Edge_Computing"), se = FALSE) +
  geom_smooth(aes(y = US_Cybersecurity_month, color = "US_Cybersecurity"), se = FALSE) +
  geom_smooth(aes(y = UX_month, color = "UX"), se = FALSE) +
  theme_classic() +
  ylab("Category Strength") +
  xlab("Time") +
  labs(color = "Legend") +
  scale_color_manual(values = colors)

ggplotly(pc_plot)
```


Include link to finished dashboard. Dashboard is not finished at this time.


## Article Roadmap

Background  
Data Acquisition, Cleaning, and Exploration  
Process  
Dashboard  


## Background

Data Science is a relatively new industry that has only recently begun to solidify. We can trace elements of Data Science far back into history, but only within the past few decades has computing technology advanced enough to make widespread statistical analyses feasible. Data Science as a field has shifted a lot over the past 20 years. Specifically, the way we talk about Data Science and the various concepts that are important.  

What is the problem? Currently, we base our thoughts on the “next big thing” in Data Science on theory. Theory in of itself is not a bad place to start, but it’s not always right when it comes to predicting changes in the industry. We need to make sure that we are prepared as industry professionals to use and understand new technology as it emerges. Being behind the curve in Data Science can seriously hamper your capabilities.  

The ultimate goal of this project is to generate an interactive dashboard using R-Shiny which allows the user to explore the most prevalent words throughout the sampled time period. Additionally, I hope to provide some general insights about how industry terminology has evolved over time on the default display of the app.  
*This should be expanded further*


## Process

I intend to scrape text data from the abstracts of a newsletter related to Data Science over a period of approximately 20 year. I'll start off by doing some more general text analysis on the article titles themselves. Then I can move into more complex methods and apply Principle Component Analysis (PCA) to aid in the analysis of each article’s abstract. I intend to generate a language consistency score which can then be visualized over time to show how industry language has shifted.  

Before I dive into any modeling, I’ll need to explore and thoroughly clean my dataset. Since I’m scraping the data from the internet I will need to do an extensive amount of text processing before it is in a useable format. Once I have everything clean, it should be fairly simple to analyze it and generate the relevant graphs.  


## Data Acquisition, Cleaning, Exploration

Quick markdown code walk-through (cliff notes version)  
*Still need to condense code to minimal levels. May be valuable to simply describe the cleaning process instead.*

Any major findings from exploration?  
The scraper did not do a perfect job collecting all the article information. There are occasional instances where nearly a month of data is missing.
*Go into more detail on what data is missing. Potentially explore* **why** *it's missing.*



## Application of method

Markdown code walk-through of method application  
*May consider just describing method and providing intermediate visuals*  

Discuss initial findings.
*Still in-progress. Would be a good idea to show trends over time here*

## Finished Dashboard

Display findings in polished graphical form  
*The* **cool** *graphs are still in development.*

Use R-Shiny to dev interactive dashboard with various graphs.
*The Shiny app is still under development and I'm not sure exactly how to integrate it into R-Markdown just yet.*  

Brief discussion on the results, ideally the visuals will do most of the talking.  
*Nothing to discuss just yet. I'm still digging into the results.*
